---
title: "Conducting and Presenting Multiple Linear Regression Analysis Using R"
author: "Jishnu Matra"
format:
  revealjs:
    theme: simple
    transition: slide
    slide-number: true
    code-fold: false
    code-line-numbers: false
    margin: 0.1
    fontsize: 28px
  beamer:
    theme: default
---

```{=html}
<style>
body {
  font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
}
.reveal pre {
  font-size: 0.55em;
}
.reveal code {
  font-size: 0.85em;
}
.reveal h2 {
  font-size: 1.5em;
  margin-bottom: 30px;
}
.reveal p, .reveal ul, .reveal ol {
  font-size: 0.85em;
}
</style>
```

## What is Linear Regression?

**Simple Linear Regression:**

- Predicts one thing based on another thing
- Example: Predicting house price based on its size
- Draws a straight line through your data that best fits the pattern
- Goal: Find the line that makes the smallest prediction errors

**Multiple Linear Regression:**

- Predicts one thing based on multiple factors
- Example: Predicting house price using size, number of rooms, age, and location
- Considers all factors together to make better predictions
- Helps you understand which factors matter most
- In R, we use the simple `lm()` function to do this

------------------------------------------------------------------------

## What is Lasso Regression?

**Lasso (Least Absolute Shrinkage and Selection Operator):**

- A smarter version of regular regression
- Automatically decides which factors are important and which aren't
- Kicks out the unimportant factors completely (sets them to zero)
- Prevents the model from being too complicated

**Key Differences from Standard lm():**

- **lm()**: Uses ALL factors you give it, even if some are useless
- **Lasso**: Automatically removes useless factors, keeps only the important ones
- **Lambda (λ)**: A dial that controls how aggressive Lasso is in removing factors
- **Benefit**: Gives you a cleaner, simpler model that's easier to understand and explain
- **Best for**: When you have many factors and aren't sure which ones really matter

------------------------------------------------------------------------

## When to Use Linear Regression

**Best For:** Continuous outcome variables (numbers on a scale)

- Sales revenue from advertising spend
- House prices based on size and location
- Temperature predictions from weather variables
- Student test scores from study hours

**Key Question It Answers:** How does one variable respond to changes in others?

**Bottom Line:**
Know your data! Linear regression needs a continuous outcome variable to work properly.

------------------------------------------------------------------------

## About the Analysis

::::: columns
::: {.column width="50%"}
**Packages Used:**

-   `glmnet` - For Lasso regression
-   `lm` - Ordinary least squares regression model 
-   `MASS` - Contains Boston dataset
:::

::: {.column width="50%"}
**Boston Housing Dataset:**

-   506 observations
-   14 variables
-   Target: `medv` (median home value)
-   Predictors: crime rate, room number, age, etc.
:::
:::::

------------------------------------------------------------------------

## Key Concepts

**Coefficients**: Numerical values that represent the relationship strength between each predictor variable and the target variable. A larger absolute value indicates stronger influence.

**R-squared (R²)**: R squared is the model's accuracy score, it ranges from 0 to 1.

**Penalty (L1)**: In Lasso, add up all the coefficient values (ignoring positive/negative signs) and "if this total gets too big, we'll add a penalty to the model." This can force some coefficients to become exactly zero, effectively performing feature selection.

**Regularization**: A technique to prevent overfitting. Without it, models can become too complicated, memorizing training data instead of learning real patterns.

**Lambda (λ)**: Lamda is a hyper parameter, which can use to control how strict and simple a model can be. If the lamda value is more, then it gives more strict penalties and keeps the model simple, and vice-versa. 

------------------------------------------------------------------------

## Step 1: Install & Load Packages

```{r}
#| echo: true
#| output: false

# Install and load packages
if (!require(glmnet)) {
  install.packages("glmnet")
}
if (!require(ggplot2)) {
  install.packages("ggplot2")
}
if (!require(reshape2)) {
  install.packages("reshape2")
}
library(glmnet)
library(MASS)
library(ggplot2)
library(reshape2)
```

**Purpose:** Set up the required libraries for our analysis

------------------------------------------------------------------------

## Step 2: Load & Prepare Data

```{r}
#| echo: true
#| output: true

# Load and prepare data
data(Boston)
head(Boston, 3)

# OR load your own CSV file from computer:
# my_data <- read.csv("path/to/your/file.csv")
# Example: my_data <- read.csv("C:/Users/YourName/Documents/data.csv")

X <- as.matrix(Boston[, -14])
y <- Boston$medv
```

**What we did:** Loaded the Boston dataset and separated predictors (X) from the target variable (y)

**To use your own data:** Replace the data loading with `read.csv()` and adjust column selection

------------------------------------------------------------------------

## Step 3: Fit Linear Model

```{r}
#| echo: true

# Fit a normal linear model
lm_model <- lm(medv ~ ., data = Boston)
lm_coef <- coef(lm_model)
lm_r_squared <- summary(lm_model)$r.squared

cat("--- Base lm() Coefficients ---\n")
print(lm_coef)
cat("\nR-squared:", lm_r_squared, "\n")
```

------------------------------------------------------------------------

## Step 4: Fit Lasso Model

```{r}
#| echo: true

# Fit a lasso model (with cross-validation)
cv_model <- cv.glmnet(X, y, alpha = 1)
best_lambda <- cv_model$lambda.min

cat("Best lambda from cross-validation:", best_lambda, "\n")

lasso_model <- glmnet(X, y, alpha = 1, lambda = best_lambda)
```

**Key Point:** Cross-validation automatically selects the optimal lambda value

------------------------------------------------------------------------

## Step 5: Extract Lasso Results

```{r}
#| echo: true

# Extract Lasso coefficients and predictions
lasso_coef <- coef(lasso_model)
lasso_predictions <- predict(lasso_model, newx = X)
lasso_r_squared <- 1 - (sum((y - lasso_predictions)^2) / 
                        sum((y - mean(y))^2))

cat("--- Lasso Coefficients ---\n")
print(lasso_coef)
```

------------------------------------------------------------------------

## Step 6: Compare Results

```{r}
#| echo: true

# R-squared Comparison
cat("--- R-squared Comparison ---\n")
cat("Base lm() R-squared:", lm_r_squared, "\n")
cat("Lasso glmnet R-squared:", lasso_r_squared, "\n")

cat("\nDifference:", lm_r_squared - lasso_r_squared, "\n")
```

**Observation:** Lasso achieves similar performance while potentially simplifying the model by shrinking some coefficients

------------------------------------------------------------------------

## Plot: Predicted vs Actual

```{r}
#| echo: true
#| fig-width: 7
#| fig-height: 6

# Predicted vs Actual values
plot(y, lasso_predictions, 
     main = "Lasso: Predicted vs Actual Values",
     xlab = "Actual Median Value ($1000s)", 
     ylab = "Predicted Median Value ($1000s)",
     pch = 19, col = rgb(0, 0, 1, 0.5))
abline(0, 1, col = "red", lwd = 2)
legend("topleft", legend = "Perfect Prediction Line", 
       col = "red", lwd = 2)
```

**What this shows:** Points close to the red line indicate accurate predictions

------------------------------------------------------------------------

## Business Applications

**Real-World Uses:**

1. **Sales Forecasting**  
   Predict future sales based on marketing spend, pricing, and seasonality to plan inventory and budgets.

2. **Customer Churn Prediction**  
   Identify which customer behaviors indicate they're likely to leave, so you can take action early.

3. **Marketing Channel Optimization**  
   Use Lasso to find which advertising channels (social media, TV, email) actually drive revenue and cut the rest.

4. **Employee Attrition Modeling**  
   Predict which employees might quit based on workload, satisfaction scores, and performance reviews.

5. **Demand Forecasting**  
   Estimate product demand using weather, promotions, and foot traffic to optimize stock levels.

6. **Loan Default Prediction**  
   Banks use regression to predict who might default on loans based on income, credit history, and debt levels.

7. **Real Estate Pricing**  
   Automatically estimate property values using location, size, age, and neighborhood features.

------------------------------------------------------------------------

## Exercise: Try With Another Dataset

**Task:**  
Run the same workflow (load → prepare → fit → compare) using a *different dataset* such as `mtcars`, `iris`, or your own CSV.

**Steps:**

1. Load a dataset: `df <- mtcars`  
2. Pick a target variable: `y <- df$mpg`  
3. Create predictors: `X <- as.matrix(df[, -1])`  
4. Fit linear & Lasso models (same code as earlier)  
5. Compare R² and identify important predictors.

**Goal:**  
See how model behavior changes with a new dataset.

------------------------------------------------------------------------

## Thank You! {.center}

**Questions?**